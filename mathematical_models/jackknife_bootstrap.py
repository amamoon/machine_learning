# -*- coding: utf-8 -*-
"""FinalQuestion1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kgt0N5FbXQB24NCI9YnFTatrtlaPXXoR
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss, accuracy_score
import matplotlib.pyplot as plt

df = pd.read_csv("Q1.csv", index_col=0)
X = df.iloc[:,:30].to_numpy()
Y = df.Y.to_numpy()
np.random.seed(125)
p = 30
k = 8
betas = np.random.random(p) + 1
betas[np.random.choice(np.arange(p), p-k, replace=False)] = 0.0

df

B = 500
np.random.seed(12)
coef_mat = np.empty(shape=(500,30))

for b in range(B):
    b_sample = np.random.choice(np.arange(X.shape[0]), size=X.shape[0])
    X_b = X[b_sample]
    Y_b = Y[b_sample]
    mod = LogisticRegression(penalty='l1', solver='liblinear', C=1000).fit(X_b, Y_b)
    coef_mat[b,:] = mod.coef_

means = np.mean(coef_mat, axis=0)
lower = np.quantile(coef_mat, 0.05, axis=0)
upper = np.quantile(coef_mat, 0.95, axis=0)
para_lower = lower
para_upper = upper
colors = ['red' if lower[i] <= 0 and upper[i] >= 0 else 'blue' for i in range(30)]

plt.vlines(x=np.arange(1,31), ymin=lower, ymax=upper, colors=colors)
plt.scatter(x=np.arange(1,31), y=means, color=colors)
plt.xlabel("$Features$")
#plt.savefig("figures/NPBootstrap.png", dpi=400)
plt.show()

#CHANGE THIS
# fit model
mod = LogisticRegression(penalty='l1', solver='liblinear', C=1000).fit(X, Y)

B = 500
np.random.seed(20)
coef_mat = np.empty(shape=(500,30))
# generate bootstrap samples
for b in range(B):
    # each bootstrap sample is same x val of old data set
    b_sample = np.random.choice(np.arange(X.shape[0]), size=X.shape[0])
    X_b = X[b_sample]
    
    # get prediction
    pred_prob = mod.predict_proba(X_b)
    prob = pred_prob[:,1]
    
    # initialise Y_b
    Y_b = np.empty(shape=Y.shape[0])
    
    # loop through each observation
    for i in range(X.shape[0]):
        p = prob[i]
        y_i = np.random.binomial(n=1, p=p, size=1)
        Y_b[i] = y_i
    
    model_bs = LogisticRegression(penalty='l1', solver='liblinear', C=1000).fit(X_b, Y_b)
    coef_mat[b,:] = model_bs.coef_
    
means = np.mean(coef_mat, axis=0)
lower = np.quantile(coef_mat, 0.05, axis=0)
upper = np.quantile(coef_mat, 0.95, axis=0)
np_lower = lower
np_upper = upper
colors = ['red' if lower[i] <= 0 and upper[i] >= 0 else 'blue' for i in range(30)]

plt.vlines(x=np.arange(1,31), ymin=lower, ymax=upper, colors=colors)
plt.scatter(x=np.arange(1,31), y=means, color=colors)
plt.xlabel("$Features$")
#plt.savefig("figures/NPBootstrap.png", dpi=400)
plt.show()

#JACKKNIFE
B = 500
np.random.seed(20)
coef_mat = np.empty(shape=(250,30))
mean_mat = np.empty(shape=(250,30))
gamma_mat = np.empty(shape=(250,30))
prior_model = LogisticRegression(penalty='l1', solver='liblinear', C=1000).fit(X, Y)
prior_coef = prior_model.coef_
X_copy = X
Y_copy = Y
n = 30
i = 0
while i < 250:
    #cut ith value
    X_j = np.delete(X_copy, i , axis=0)
    Y_j = np.delete(Y_copy, i,  axis=0)
    # refit on new data
    new_model = LogisticRegression(penalty='l1', solver='liblinear', C=1000).fit(X_j, Y_j)
    # denote estimated coef vector
    jack_coef = new_model.coef_
    coef_mat[i,:] = jack_coef
    # define ith psuedo value as gamma
    gamma = (n*prior_coef) - ((n-1)*jack_coef)
    gamma_mat[i,:] = gamma
    # compute mean and variance
    i += 1

gamma_means = np.mean(gamma_mat, axis=0)
gamma_vars  = np.var(gamma_mat, axis=0)
ci_lower = gamma_means - 1.645*(np.sqrt((gamma_vars/n)))
ci_upper = gamma_means + 1.645*(np.sqrt((gamma_vars/n)))
jk_lower = ci_lower
jk_upper = ci_upper
colors = ['red' if ci_lower[i] <= 0 and ci_upper[i] >= 0 else 'blue' for i in range(30)]

plt.vlines(x=np.arange(1,31), ymin=ci_lower, ymax=ci_upper, colors=colors)
plt.scatter(x=np.arange(1,31), y=gamma_means, color=colors)
plt.xlabel("$Features$")
#plt.savefig("figures/NPBootstrap.png", dpi=400)
plt.show()

np.random.seed(125)
p = 30
k = 8
betas = np.random.random(p) + 1
betas[np.random.choice(np.arange(p), p-k, replace=False)] = 0.0

para_contains = [0 if para_lower[i] <= 0 and para_upper[i] >= 0 else 1 for i in range(30)]
np_contains = [0 if np_lower[i] <= 0 and np_upper[i] >= 0 else 1 for i in range(30)]
jk_contains = [0 if jk_lower[i] <= 0 and jk_upper[i] >= 0 else 1 for i in range(30)]
beta_contains = [0 if betas[i] == 0 else 1 for i in range(len(betas))]

para_fp = 0
para_fn = 0

np_fp = 0
np_fn = 0

jk_fp = 0
jk_fn = 0

def cal_fp_fn(true_vals, test_vals):
    test_fp = 0
    test_fn = 0
    for i in range(30):
        true_val = true_vals[i]
        test_val = test_vals[i]
        if test_val == 1 and true_val == 0:
            test_fp += 1
        elif test_val == 0 and true_val == 1:
            test_fn += 1
    return test_fp, test_fn

para_fp, para_fn = cal_fp_fn(beta_contains, para_contains)
np_fp, np_fn = cal_fp_fn(beta_contains, np_contains)
jk_fp, jk_fn = cal_fp_fn(beta_contains, jk_contains)

column = ["Algorithm", "# False Positives", "# False Negatives"]
fp_vals = [np_fp, para_fp, jk_fp]
fn_vals = [np_fn, para_fn, jk_fn]
algorithms = ["NP","P","JK"]
vals = {"Algorithm":algorithms, "# False Positives":fp_vals, "# False Negatives":fn_vals}
d_df = pd.DataFrame(vals, columns=column)
print(d_df)

star = "*"*10
line = "\n"
print(star + line + "BETAS" + line + star)
print(beta_contains)
print(star + line + "PARAMETRIC CIs" + line + star)
print(para_contains)
print(star + line + "NON-PARAMETRIC CIs" + line + star)
print(np_contains)
print(star + line + "JACK KNIFE CIs" + line + star)
print(jk_contains)
